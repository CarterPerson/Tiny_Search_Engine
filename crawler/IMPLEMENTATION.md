// authored by Carter Person, May 6th, 2020 for lab4 of CS50

I implemented this crawler by using multiple different datasets, though I pretty much followed the psuedocode that was offered to use on the course webpage. I started out by ensuring that all of the inputs were valid inputs, and then once I checked that, I progress to move onto the actual crawler function. In the crawler function, I used a datastructure called "bag_t", which effectively functions as a stack, to store all of the webpages I had yet to process and make files for. I made a web_page holding the initial URL with a depth of 0, and then progressed through the processing of the webpages. For each webpage, I started off by pulling it's html code, and then I wrote a file that holds its url, depth, and html code into the target directory. During this process, I ensured that certain actions, such as the pulling of html code, were performed successfully to avoid crashing. After the creation of the file with that pages data, I checked to see whether I had reached the maximum depth I was searching for. If it was not as deep as the limit the user passed in, I would then scan the html code for urls. For each url, after checking that it was within the acceptable domain, I then created a new webpage object with the depth of its "parent" webpage +1 and added it to the bag of webpages I still needed to go through. This essentially made a depth-first search kind of process to cycle through all of the URLS down to a certain depth.


 As I went through, I made sure to keep track of the URLs that I had looked at as well. This is where the hashtable comes into play. I stored each URL i had already visited as a key in the hashtable to have a more optimized storage method than a typical set. Whenever I found a new URL, I attempted to add it to the hashtable. Only if it was accepted, meaning that it did not already exist in the hashtable, would I create a webpage object and continue to add it to the bag of webpages still to be saved.